# MedMT: Medical Dialogue Machine Translation (Chineseâ†’Thai)

<p align="center" style="position: relative;">
    <img src="assets/logo.png" alt="MedMT Logo" width="200" style="position: relative; z-index: 1;"/>
</p>

<p align="center">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
        <img src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg" alt="License: CC BY-NC-SA 4.0"/>
    </a>
</p>

## ğŸ“– Overview

MedMT is a specialized machine translation system for medical dialogues between Chinese and Thai languages. Using advanced AI technologies and context-aware translation, it aims to preserve both medical accuracy and conversational nuances.

## ğŸ†• Latest Updates

- **Multi-turn Dialogue Generation**: New mode for generating realistic multi-turn conversations between doctors and patients
- **Automatic Quality Assessment**: Built-in tools for translation quality evaluation
- **Response Caching System**: Improved efficiency through API response caching
- **Dataset Analysis Tools**: Comprehensive analysis of dataset diversity and quality metrics
- **DeepSeek Reasoner Integration**: Enhanced dataset generation using DeepSeek Reasoner model for higher quality translations
- **XmodelLM1.5 Support**: Added custom implementation for loading and using the XiaoduoAILab/XmodelLM1.5 model
- **Improved Dataset Diversity**: Enhanced prompt engineering and parameter strategies for more diverse medical dialogues
- **Model Comparison Tools**: Added scripts to compare translations between different models

## âœ¨ Key Features

- **Context-aware Translation**: Leverages multi-turn dialogue context
- **Medical Accuracy**: Preserves medical terminology and meaning
- **Synthetic Data**: High-quality synthetic dialogues using DeepSeek AI
- **Multiple Formats**: Supports dialogue, Q&A, and multi-turn conversation formats
- **Quality Assessment**: Automatic evaluation of translation quality
- **Dataset Analysis**: Tools to analyze dataset diversity and characteristics
- **Advanced Training**: LoRA/PEFT for efficient fine-tuning
- **Scalable**: DeepSpeed/Accelerate for multi-GPU training
- **Multi-Model Support**: Integration with DeepSeek Reasoner and XiaoduoAILab/XmodelLM1.5

## ğŸ› ï¸ Components

### 1. Data Generation

- Uses DeepSeek Reasoner for enhanced synthetic data generation
- Supports dialogue, Q&A, and multi-turn conversation formats
- Includes automatic quality assessment
- Response caching for improved efficiency
- Dataset analysis tools for quality control
- Includes metadata and licensing information
- Licensed under CC BY-SA-NC 4.0

### 2. Model Development

- Based on multiple pretrained models (mBART, NLLB, XmodelLM1.5)
- Fine-tuned with LoRA/PEFT
- Scaled with DeepSpeed/Accelerate
- Context-aware translation capabilities

### 3. Evaluation System

- Automatic metrics evaluation (BLEU, METEOR)
- Human evaluation support
- Quality assurance checks
- Model comparison tools

### 4. Deployment Options

- Hugging Face Hub integration
- Local inference capabilities
- Easy-to-use API
- Custom model loading for specialized models

## ğŸš€ Getting Started

### System Requirements

- Python 3.8+
- CUDA-compatible GPU (recommended for training)
- 16GB RAM minimum (32GB recommended for training)
- 100GB disk space for datasets and models

### Dependencies

```bash
# Core ML libraries
torch>=1.13
transformers==4.40.0
datasets>=2.19.0
sentencepiece
sacrebleu

# Training utilities
accelerate
peft
deepspeed
safetensors
huggingface_hub

# Data processing
pandas
scikit-learn
tqdm
openpyxl

# Development
jupyter
```

### Installation

```bash
git clone https://github.com/JonusNattapong/medmt.git
cd medmt
pip install -r requirements.txt
```

### Generate Dataset

```bash
# Generate dialogue dataset
python src/generate_dataset.py --output data/dialogue_train.csv --n_samples 1000 --mode dialogue

# Generate QA dataset
python src/generate_dataset.py --output data/qa_train.csv --n_samples 1000 --mode qa

# Generate multi-turn dialogue dataset
python src/generate_dataset.py --output data/multiturn_train.csv --n_samples 50 --mode multiturn

# Generate dataset with quality assessment
python src/generate_dataset.py --output data/qa_train.csv --n_samples 1000 --mode qa --assess_quality

# Use response caching to improve efficiency
python src/generate_dataset.py --output data/dialogue_train.csv --n_samples 1000 --use_cache

# Analyze an existing dataset
python src/analyze_dataset.py data/dialogue_train.csv --output-dir data/analysis
```

### Training

```bash
# Basic training
python src/train.py --config config.yaml

# Training with DeepSpeed
deepspeed src/train.py --config config.yaml --deepspeed ds_config.json

# Training with LoRA
python src/train.py --config config.yaml --use_lora
```

### Evaluation

```bash
python src/evaluate.py --config config.yaml
```

### Inference

```bash
python src/inference.py --input data/test.csv --output predictions.csv
```

### Using XiaoduoAILab/XmodelLM1.5

```bash
# Run a simple translation example
python examples/xiaoduo_translation_example.py

# Compare translations between DeepSeek Reasoner and XmodelLM1.5
python examples/model_comparison.py

# Evaluate translation quality against a reference dataset
python examples/evaluate_translations.py --dataset data/test_data.csv --output evaluation_results.csv
```

For more details on using the XmodelLM1.5 model, see the `examples/README.md` file.

## ğŸ“Š Dataset Format

### Dialogue Format

```python
{
    "context": "åŒ»ç”Ÿï¼šä½ æœ€è¿‘è¡€ç³–æ§åˆ¶å¾—æ€ä¹ˆæ ·ï¼Ÿ\nç—…äººï¼šæœ‰ç‚¹é«˜ã€‚",
    "source": "ä½ æœ€è¿‘é¥®é£Ÿæœ‰æ³¨æ„å—ï¼Ÿ",
    "target": "à¸Šà¹ˆà¸§à¸‡à¸™à¸µà¹‰à¸„à¸¸à¸“à¸„à¸§à¸šà¸„à¸¸à¸¡à¸­à¸²à¸«à¸²à¸£à¸”à¸µà¹„à¸«à¸¡?"
}
```

### QA Format

```python
{
    "context": "æ‚£è€…æœ€è¿‘è¡€ç³–åé«˜ï¼Œä¸”æœ‰å£æ¸´ã€å¤šå°¿ç­‰ç—‡çŠ¶ã€‚",
    "question_zh": "è¿™å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿéœ€è¦åšä»€ä¹ˆæ£€æŸ¥ï¼Ÿ",
    "answer_zh": "ä»ç—‡çŠ¶æ¥çœ‹ï¼Œå¯èƒ½æ˜¯ç³–å°¿ç—…ã€‚å»ºè®®åšç©ºè…¹è¡€ç³–å’Œç³–åŒ–è¡€çº¢è›‹ç™½æ£€æŸ¥ã€‚",
    "question_th": "à¸­à¸²à¸à¸²à¸£à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¹‚à¸£à¸„à¸­à¸°à¹„à¸£? à¸„à¸§à¸£à¸•à¸£à¸§à¸ˆà¸­à¸°à¹„à¸£à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡?",
    "answer_th": "à¸ˆà¸²à¸à¸­à¸²à¸à¸²à¸£à¸—à¸µà¹ˆà¸à¸š à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¹‚à¸£à¸„à¹€à¸šà¸²à¸«à¸§à¸²à¸™ à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¸•à¸£à¸§à¸ˆà¸™à¹‰à¸³à¸•à¸²à¸¥à¹ƒà¸™à¹€à¸¥à¸·à¸­à¸”à¸‚à¸“à¸°à¸­à¸”à¸­à¸²à¸«à¸²à¸£à¹à¸¥à¸°à¸„à¹ˆà¸² HbA1c"
}
```

### Multi-turn Dialogue Format

```python
{
    "context": "æ‚£è€…æœ‰å¤´ç—›ç—‡çŠ¶ï¼Œéœ€è¦åŒ»ç–—å»ºè®®ã€‚",
    "topic_zh": "å¤´ç—›",
    "topic_th": "à¸›à¸§à¸”à¸¨à¸µà¸£à¸©à¸°",
    "turn_number": 1,
    "patient_zh": "æˆ‘æœ€è¿‘ç»å¸¸å¤´ç—›ï¼Œç‰¹åˆ«æ˜¯æ—©ä¸Šèµ·åºŠåã€‚",
    "patient_th": "à¸Šà¹ˆà¸§à¸‡à¸™à¸µà¹‰à¸‰à¸±à¸™à¸›à¸§à¸”à¸«à¸±à¸§à¸šà¹ˆà¸­à¸¢ à¹‚à¸”à¸¢à¹€à¸‰à¸à¸²à¸°à¸«à¸¥à¸±à¸‡à¸•à¸·à¹ˆà¸™à¸™à¸­à¸™à¸•à¸­à¸™à¹€à¸Šà¹‰à¸²",
    "doctor_zh": "æ‚¨çš„å¤´ç—›æ˜¯æŒç»­æ€§çš„è¿˜æ˜¯é˜µå‘æ€§çš„ï¼Ÿç–¼ç—›ç¨‹åº¦å¦‚ä½•ï¼Ÿ",
    "doctor_th": "à¸­à¸²à¸à¸²à¸£à¸›à¸§à¸”à¸«à¸±à¸§à¸‚à¸­à¸‡à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™à¹à¸šà¸šà¸•à¹ˆà¸­à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸«à¸£à¸·à¸­à¹€à¸›à¹‡à¸™à¸à¸±à¸à¹†? à¸„à¸§à¸²à¸¡à¸£à¸¸à¸™à¹à¸£à¸‡à¸‚à¸­à¸‡à¸­à¸²à¸à¸²à¸£à¹€à¸›à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡?"
}
```

## ğŸ“„ License

This project is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License ([CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)).

- **Attribution**: You must give appropriate credit
- **NonCommercial**: For research and non-commercial use only
- **ShareAlike**: If you remix, transform, or build upon the material, you must distribute your contributions under the same license

## ğŸ‘¥ Contributors

- zombitx64 : <zombitx64@gmail.com>

## ğŸ“š References

- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
- [DeepSeek AI](https://deepseek.ai/)
- [LoRA/PEFT](https://github.com/huggingface/peft)
- [DeepSpeed](https://www.deepspeed.ai/)

## ğŸ“š Citation

```bibtex
@misc{wang2024xmodellm1.5,
    title={Xmodel-LM1.5: An 1B-scale Multilingual LLM},
    author={Qun Wang and Yang Liu and QingQuan Lin  and Ling Jiang},
    year={2024},
    eprint={2411.10083},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

---
[Read the Xmodel-LM1.5 paper on arXiv](https://arxiv.org/pdf/2411.10083)
> **MedMT** Â© 2025 | Released under CC BY-NC-SA 4.0 | For research purposes only

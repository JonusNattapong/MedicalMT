{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65c547f",
   "metadata": {},
   "source": [
    "# MedMT: Training & Inference on Colab\n",
    "โน้ตบุ๊กนี้สำหรับรันเทรนและอินเฟอเรนซ์โมเดล MedMT บน Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339d2e3",
   "metadata": {},
   "source": [
    "## 1. ติดตั้ง dependencies\n",
    "ติดตั้งไลบรารีที่จำเป็นทั้งหมดสำหรับโปรเจกต์นี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch>=1.13 transformers==4.40.0 datasets>=2.19.0 sentencepiece sacrebleu pandas scikit-learn accelerate tqdm openpyxl jupyter safetensors huggingface_hub peft deepspeed tenacity>=8.2.0 python-dotenv>=1.0.0 openai>=1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008faa03",
   "metadata": {},
   "source": [
    "## 2. อัปโหลดไฟล์โค้ดและไฟล์ข้อมูล\n",
    "อัปโหลดไฟล์ในโฟลเดอร์ src/ และไฟล์ข้อมูลที่จำเป็น เช่น config.yaml, train.csv, test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ac6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314628d",
   "metadata": {},
   "source": [
    "## 3. เทรนโมเดล (train.py)\n",
    "ตัวอย่างโค้ดสำหรับเทรนโมเดล MedMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0be7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "from src.data_loader import load_data\n",
    "from src.model import MedMTModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo\n",
    "\n",
    "class MedDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx]['source'], self.df.iloc[idx]['target']\n",
    "\n",
    "def train(config):\n",
    "    df = load_data(config['train_data'], is_train=True)\n",
    "    model = MedMTModel(config['pretrained_model'])\n",
    "    optimizer = AdamW(model.model.parameters(), lr=config['learning_rate'])\n",
    "    dataset = MedDataset(df)\n",
    "    loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    model.model.train()\n",
    "    for epoch in range(config['epochs']):\n",
    "        for src, tgt in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = model.tokenizer(list(src), return_tensors=\"pt\", padding=True, truncation=True, max_length=config['max_seq_length']).to(model.device)\n",
    "            labels = model.tokenizer(list(tgt), return_tensors=\"pt\", padding=True, truncation=True, max_length=config['max_seq_length']).input_ids.to(model.device)\n",
    "            outputs = model.model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n",
    "    os.makedirs(os.path.dirname(config['model_save_path']), exist_ok=True)\n",
    "    model.model.save_pretrained(config['model_save_path'])\n",
    "    model.tokenizer.save_pretrained(config['model_save_path'])\n",
    "    try:\n",
    "        from safetensors.torch import save_file as save_safetensors\n",
    "        import torch\n",
    "        save_safetensors(model.model.state_dict(), os.path.join(config['model_save_path'], 'model.safetensors'))\n",
    "        print(f\"Model weights saved as .safetensors at {config['model_save_path']}\")\n",
    "    except ImportError:\n",
    "        print(\"safetensors not installed, skipping .safetensors save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab31ee1",
   "metadata": {},
   "source": [
    "## 4. อินเฟอเรนซ์ (inference.py)\n",
    "ตัวอย่างโค้ดสำหรับรันอินเฟอเรนซ์โมเดล MedMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from src.data_loader import load_data\n",
    "from src.model import MedMTModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def inference(config, input_path, output_path):\n",
    "    df = load_data(input_path, is_train=False)\n",
    "    model = MedMTModel(config['model_save_path'])\n",
    "    srcs = df['source'].tolist()\n",
    "    preds = []\n",
    "    for src in tqdm(srcs, desc=\"Translating\"):\n",
    "        pred = model.translate([src])[0]\n",
    "        preds.append(pred)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    pd.DataFrame({'id': range(len(preds)), 'translation': preds}).to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6e192",
   "metadata": {},
   "source": [
    "## 5. ตัวอย่างการใช้งาน\n",
    "ตัวอย่างการเรียกใช้ฟังก์ชัน train และ inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c04028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# โหลด config\n",
    "with open('config.yaml', 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "# เทรนโมเดล\n",
    "# train(config)\n",
    "# อินเฟอเรนซ์\n",
    "# inference(config, 'data/test.csv', 'output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- src/data_loader.py ---\n",
    "import pandas as pd\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import HfApi, HfFolder, create_repo, Repository\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "def load_data(path: str, is_train: bool = True):\n",
    "    df = pd.read_csv(path)\n",
    "    if is_train:\n",
    "        assert set(['context', 'source', 'target']).issubset(df.columns)\n",
    "    else:\n",
    "        assert set(['context', 'source']).issubset(df.columns)\n",
    "    return df\n",
    "\n",
    "def generate_synthetic_medical_dialogue(n_samples=1000, seed=42):\n",
    "    random.seed(seed)\n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        context = \"แพทย์: คุณมีอาการอะไรบ้าง?\\nผู้ป่วย: ฉันปวดหัว\"\n",
    "        source = random.choice([\"คุณกินยาหรือยัง?\", \"คุณต้องพักผ่อน\", \"คุณต้องตรวจเพิ่มเติม\", \"คุณมีไข้ไหม?\"])\n",
    "        target = random.choice([\"คุณกินยาแล้วหรือยัง?\", \"คุณควรพักผ่อน\", \"คุณต้องตรวจร่างกาย\", \"คุณมีไข้ไหม?\"])\n",
    "        data.append({\"context\": context, \"source\": source, \"target\": target})\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def save_dataset_to_hub(df, repo_name, split_name=\"train\", private=False):\n",
    "    api = HfApi()\n",
    "    token = HfFolder.get_token()\n",
    "    if token is None:\n",
    "        raise RuntimeError(\"Please login to Hugging Face CLI: huggingface-cli login\")\n",
    "    repo_url = create_repo(repo_name, token=token, private=private, exist_ok=True)\n",
    "    local_dir = f\"./tmp_{repo_name.replace('/', '_')}\"\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset.save_to_disk(local_dir)\n",
    "    repo = Repository(local_dir=local_dir, clone_from=repo_url.repo_url, use_auth_token=token)\n",
    "    repo.git_add()\n",
    "    repo.git_commit(\"Add synthetic dataset\")\n",
    "    repo.git_push()\n",
    "    print(f\"Dataset pushed to Hugging Face Hub: {repo_url.repo_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- src/model.py ---\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from safetensors.torch import save_file as save_safetensors\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "class MedMTModel:\n",
    "    def __init__(self, model_name_or_path: str, device: Optional[str] = None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def translate(self, src_texts, max_length=256):\n",
    "        inputs = self.tokenizer(src_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(self.device)\n",
    "        outputs = self.model.generate(**inputs, max_length=max_length)\n",
    "        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    def save(self, save_dir: str, use_safetensors: bool = False):\n",
    "        save_dir = os.path.join(os.getcwd(), 'models')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.tokenizer.save_pretrained(save_dir)\n",
    "        if use_safetensors:\n",
    "            weights_path = os.path.join(save_dir, \"model.safetensors\")\n",
    "            save_safetensors(self.model.state_dict(), weights_path)\n",
    "        else:\n",
    "            self.model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- src/evaluate.py ---\n",
    "import argparse\n",
    "import yaml\n",
    "from src.data_loader import load_data\n",
    "from src.model import MedMTModel\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(config):\n",
    "    df = load_data(config['train_data'], is_train=True)\n",
    "    model = MedMTModel(config['model_save_path'])\n",
    "    srcs = df['source'].tolist()\n",
    "    tgts = df['target'].tolist()\n",
    "    preds = []\n",
    "    for src in tqdm(srcs, desc=\"Evaluating\"):\n",
    "        pred = model.translate([src])[0]\n",
    "        preds.append(pred)\n",
    "    bleu = sacrebleu.corpus_bleu(preds, [tgts], smooth_method='exp')\n",
    "    print(f\"BLEU: {bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- src/generate_dataset.py ---\n",
    "# หมายเหตุ: โค้ดนี้ยาวมากและมีฟังก์ชันย่อยจำนวนมาก สามารถนำเข้าไฟล์นี้หรือคัดลอกฟังก์ชันที่ต้องการใช้งานได้\n",
    "# ตัวอย่างการเรียกใช้งาน (แนะนำให้รันใน Colab เฉพาะฟังก์ชันที่ต้องการ)\n",
    "# from src.generate_dataset import generate_deepseek_medical_dialogue\n",
    "# df = generate_deepseek_medical_dialogue(n_samples=10)\n",
    "# df.head()\n",
    "# สามารถดูรายละเอียดฟังก์ชันอื่น ๆ ได้ในไฟล์ generate_dataset.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- src/train.py ---\n",
    "import argparse\n",
    "import yaml\n",
    "from src.data_loader import load_data\n",
    "from src.model import MedMTModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo\n",
    "\n",
    "class MedDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx]['source'], self.df.iloc[idx]['target']\n",
    "\n",
    "def train(config):\n",
    "    df = load_data(config['train_data'], is_train=True)\n",
    "    model = MedMTModel(config['pretrained_model'])\n",
    "    optimizer = AdamW(model.model.parameters(), lr=config['learning_rate'])\n",
    "    dataset = MedDataset(df)\n",
    "    loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    model.model.train()\n",
    "    for epoch in range(config['epochs']):\n",
    "        for src, tgt in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs = model.tokenizer(list(src), return_tensors=\"pt\", padding=True, truncation=True, max_length=config['max_seq_length']).to(model.device)\n",
    "            labels = model.tokenizer(list(tgt), return_tensors=\"pt\", padding=True, truncation=True, max_length=config['max_seq_length']).input_ids.to(model.device)\n",
    "            outputs = model.model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n",
    "    os.makedirs(os.path.dirname(config['model_save_path']), exist_ok=True)\n",
    "    model.model.save_pretrained(config['model_save_path'])\n",
    "    model.tokenizer.save_pretrained(config['model_save_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- src/inference.py ---\n",
    "import argparse\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from src.data_loader import load_data\n",
    "from src.model import MedMTModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def inference(config, input_path, output_path):\n",
    "    df = load_data(input_path, is_train=False)\n",
    "    model = MedMTModel(config['model_save_path'])\n",
    "    srcs = df['source'].tolist()\n",
    "    preds = []\n",
    "    for src in tqdm(srcs, desc=\"Translating\"):\n",
    "        pred = model.translate([src])[0]\n",
    "        preds.append(pred)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    pd.DataFrame({'id': range(len(preds)), 'translation': preds}).to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- translate_med_dialogue.py ---\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_metric, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,\n",
    "                          Seq2SeqTrainingArguments, Seq2SeqTrainer)\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Omni-7B\"\n",
    "\n",
    "def load_data(train_path, test_path, context_col=\"Context\", source_col=\"Source\", target_col=\"Target\"):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "def preprocess_function(examples, tokenizer, context_col, source_col, target_col=None, max_length=256):\n",
    "    src_texts = [\n",
    "        (c + \"\\n\" if c and not pd.isna(c) else \"\") + s\n",
    "        for c, s in zip(examples[context_col], examples[source_col])\n",
    "    ]\n",
    "    model_inputs = tokenizer(src_texts, max_length=max_length, truncation=True)\n",
    "    if target_col and target_col in examples:\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples[target_col], max_length=max_length, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def train_model(train_file, output_dir, context_col=\"Context\", source_col=\"Source\", target_col=\"Target\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    tokenized_train = train_dataset.map(lambda x: preprocess_function(x, tokenizer, context_col, source_col, target_col), batched=True)\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"no\",\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        save_total_limit=1,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        predict_with_generate=True,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        learning_rate=5e-5,\n",
    "        report_to=[],\n",
    "    )\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "def evaluate_model(model_dir, test_file, context_col=\"Context\", source_col=\"Source\", target_col=\"Target\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    metric = load_metric(\"sacrebleu\")\n",
    "    preds = []\n",
    "    refs = []\n",
    "    for i, row in test_df.iterrows():\n",
    "        src = (str(row[context_col]) + \"\\n\" if context_col in row and pd.notna(row[context_col]) else \"\") + str(row[source_col])\n",
    "        input_ids = tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=256).input_ids\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(input_ids, max_length=256, num_beams=4)\n",
    "        pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        preds.append(pred)\n",
    "        refs.append([str(row[target_col])])\n",
    "    bleu = metric.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    print(f\"BLEU score: {bleu:.2f}\")\n",
    "    return bleu\n",
    "\n",
    "def translate_file(model_dir, input_file, output_file, context_col=\"Context\", source_col=\"Source\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    df = pd.read_csv(input_file)\n",
    "    translations = []\n",
    "    for i, row in df.iterrows():\n",
    "        src = (str(row[context_col]) + \"\\n\" if context_col in row and pd.notna(row[context_col]) else \"\") + str(row[source_col])\n",
    "        input_ids = tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=256).input_ids\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(input_ids, max_length=256, num_beams=4)\n",
    "        pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        translations.append(pred)\n",
    "    df[\"Prediction\"] = translations\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved translations to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
